model: "meta-llama/Meta-Llama-3.1-8B"
model_short: 'llama'

dataset: 
  train: /scratch/chaijy_root/chaijy2/shuyuwu/llama31_last_token_hiddens
  test: /scratch/chaijy_root/chaijy2/shuyuwu/llama31_last_token_hiddens_test

mdl:
  batch_size: 16
  probe_type: "mlp"      # "mlp", "linear", "multilinear"
  # probe_hidden_size: only save hidden size as a list, as the input and output sizes are inferred. Only valid for mlp and multilinear probes. 
  probe_hidden_size: [16]
  train_epochs: 10
  lr: 1e-3
  seed: 42
  early_stopping_gap: 3

hydra:
  run:
    dir: outputs/${model_short}/${mdl.probe_type}_${now:%m%d_%H%M%S}
